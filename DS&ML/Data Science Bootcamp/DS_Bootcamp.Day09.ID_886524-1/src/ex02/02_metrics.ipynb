{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 09. Exercise 02\n",
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the same dataframe as in the previous exercise.\n",
    "2. Using `train_test_split` with parameters `test_size=0.2`, `random_state=21` get `X_train`, `y_train`, `X_test`, `y_test`. Use the additional parameter `stratify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/day-of-week-not-scaled.csv\")\n",
    "df[\"dayofweek\"] = pd.read_csv(\"../data/dayofweek.csv\", usecols=[\"dayofweek\"])\n",
    "\n",
    "x = df.drop(columns=\"dayofweek\")\n",
    "y = df[\"dayofweek\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the best parameters from the previous exercise and train the model of SVM.\n",
    "2. You need to calculate `accuracy`, `precision`, `recall`, `ROC AUC`.\n",
    "\n",
    " - `precision` and `recall` should be calculated for each class (use `average='weighted'`)\n",
    " - `ROC AUC` should be calculated for each class against any other class (all possible pairwise combinations) and then weighted average should be applied for the final metric\n",
    " - the code in the cell should display the result as below:\n",
    "\n",
    "```\n",
    "accuracy is 0.88757\n",
    "precision is 0.89267\n",
    "recall is 0.88757\n",
    "roc_auc is 0.97878\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.88757\n",
      "precision is 0.89267\n",
      "recall is 0.88757\n",
      "roc_auc is 0.97878\n"
     ]
    }
   ],
   "source": [
    "model = SVC(random_state=21, C=10, class_weight=None, gamma=\"auto\", kernel=\"rbf\", probability=True)\n",
    "model.fit(x_train, y_train)\n",
    "predict = model.predict(x_test)\n",
    "print(f\"accuracy is {accuracy_score(y_test, predict):.5f}\")\n",
    "print(f\"precision is {precision_score(y_test, predict, average='weighted'):.5f}\")\n",
    "print(f\"recall is {recall_score(y_test, predict, average='weighted'):.5f}\")\n",
    "\n",
    "predict_proba = model.predict_proba(x_test)\n",
    "print(f\"roc_auc is {roc_auc_score(y_test, predict_proba, multi_class='ovo', average='weighted'):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The same task for decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.88462\n",
      "precision is 0.88765\n",
      "recall is 0.88462\n",
      "roc_auc is 0.93528\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=21, class_weight=\"balanced\", criterion=\"gini\", max_depth=21)\n",
    "model.fit(x_train, y_train)\n",
    "predict = model.predict(x_test)\n",
    "print(f\"accuracy is {accuracy_score(y_test, predict):.5f}\")\n",
    "print(f\"precision is {precision_score(y_test, predict, average='weighted'):.5f}\")\n",
    "print(f\"recall is {recall_score(y_test, predict, average='weighted'):.5f}\")\n",
    "\n",
    "predict_proba = model.predict_proba(x_test)\n",
    "print(f\"roc_auc is {roc_auc_score(y_test, predict_proba, multi_class='ovo', average='weighted'):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The same task for random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.92604\n",
      "precision is 0.92754\n",
      "recall is 0.92604\n",
      "roc_auc is 0.98939\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=21, class_weight=\"balanced\", criterion=\"entropy\", max_depth=24, n_estimators=100)\n",
    "model.fit(x_train, y_train)\n",
    "predict = model.predict(x_test)\n",
    "print(f\"accuracy is {accuracy_score(y_test, predict):.5f}\")\n",
    "print(f\"precision is {precision_score(y_test, predict, average='weighted'):.5f}\")\n",
    "print(f\"recall is {recall_score(y_test, predict, average='weighted'):.5f}\")\n",
    "\n",
    "predict_proba = model.predict_proba(x_test)\n",
    "print(f\"roc_auc is {roc_auc_score(y_test, predict_proba, multi_class='ovo', average='weighted'):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose the best model.\n",
    "2. Analyze: for which `weekday` your model makes the most errors (in % of the total number of samples of that class in your full dataset), for which `labname` and for which `users`.\n",
    "3. Save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=21, class_weight=\"balanced\", criterion=\"entropy\", max_depth=24, n_estimators=100)\n",
    "model.fit(x_train, y_train)\n",
    "predict = model.predict(x)\n",
    "df[\"predict\"] = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.003559\n",
       "5    0.002966\n",
       "1    0.002372\n",
       "4    0.001779\n",
       "3    0.001779\n",
       "6    0.001186\n",
       "2    0.001186\n",
       "Name: dayofweek, dtype: float64"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.predict != df.dayofweek].dayofweek.value_counts() / len(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = df[df[\"predict\"] != df[\"dayofweek\"]]\n",
    "users = [i for i in df.columns if i.startswith(\"uid_\")]\n",
    "labnames = [i for i in df.columns if i.startswith(\"labname_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max user error: uid_user_2, error percent: 0.178%\n"
     ]
    }
   ],
   "source": [
    "max_error = 0\n",
    "max_user = ''\n",
    "for user in users:\n",
    "    error_perc = error[user].sum() / len(predict)\n",
    "    if error_perc > max_error:\n",
    "        max_error = error_perc\n",
    "        max_user = user\n",
    "print(f\"max user error: {max_user}, error percent: {(max_error * 100):.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max lab error: labname_project1, error percent: 0.593%\n"
     ]
    }
   ],
   "source": [
    "max_error = 0\n",
    "max_user = ''\n",
    "for lab in labnames:\n",
    "    error_perc = error[lab].sum() / len(predict)\n",
    "    if error_perc > max_error:\n",
    "        max_error = error_perc\n",
    "        max_lab = lab\n",
    "print(f\"max lab error: {max_lab}, error percent: {(max_error * 100):.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(model, \"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function that takes a list of different models and a corresponding list of parameters (dicts) and returns a dict that contains all the 4 metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models(models, params):\n",
    "    results = {}\n",
    "    for model_, param in zip(models, params):\n",
    "\n",
    "        model = model_(**param)\n",
    "        model.fit(x_train, y_train)\n",
    "        predict = model.predict(x_test)\n",
    "\n",
    "        predict_proba = model.predict_proba(x_test)\n",
    "\n",
    "        results[model.__class__.__name__] = {\n",
    "            \"accuracy\": accuracy_score(y_test, predict),\n",
    "            \"precision\": precision_score(y_test, predict, average=\"weighted\"),\n",
    "            \"recall\": recall_score(y_test, predict, average=\"weighted\"),\n",
    "            \"roc_auc\": roc_auc_score(y_test, predict_proba, multi_class=\"ovo\", average=\"weighted\")\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RandomForestClassifier': {'accuracy': 0.9260355029585798,\n",
       "  'precision': 0.9275374670957044,\n",
       "  'recall': 0.9260355029585798,\n",
       "  'roc_auc': 0.9893851880258296},\n",
       " 'SVC': {'accuracy': 0.8875739644970414,\n",
       "  'precision': 0.8926729169690374,\n",
       "  'recall': 0.8875739644970414,\n",
       "  'roc_auc': 0.9787793228216216},\n",
       " 'DecisionTreeClassifier': {'accuracy': 0.8846153846153846,\n",
       "  'precision': 0.8876518218623483,\n",
       "  'recall': 0.8846153846153846,\n",
       "  'roc_auc': 0.935280206669359}}"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [RandomForestClassifier, SVC, DecisionTreeClassifier]\n",
    "params = [\n",
    "    {\"class_weight\": \"balanced\",\n",
    "        \"criterion\": \"entropy\",\n",
    "        \"max_depth\": 24,\n",
    "        \"n_estimators\": 100, \"random_state\": 21},\n",
    "    {\"C\": 10, \"class_weight\": None, \"gamma\": \"auto\", \"kernel\": \"rbf\", \"probability\": True, \"random_state\": 21},\n",
    "    {\"class_weight\": \"balanced\",\n",
    "    \"criterion\": \"gini\",\n",
    "    \"max_depth\": 21,\n",
    "    \"random_state\": 21}\n",
    "]\n",
    "\n",
    "list_models(models, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
